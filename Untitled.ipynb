{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev)+b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z, activation):\n",
    "    #if activation == 'sigmoid':\n",
    "      #  A = 1/(1+np.exp(-Z))\n",
    "\n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(Z, 0)\n",
    "        \n",
    "    if activation == 'softmax':\n",
    "        A = np.exp(Z)\n",
    "        sum = A.sum(axis = 0, keepdims=True)\n",
    "        A = A/sum\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00235563 0.00235563 0.00235563]\n",
      " [0.04731416 0.04731416 0.04731416]\n",
      " [0.95033021 0.95033021 0.95033021]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "print(A)\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "print(A)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, label):\n",
    "\n",
    "    m = AL.shape[1]\n",
    "   # cost = -1/m*(np.sum(label*np.log(AL)+(1-label)*np.log(1-AL)))\n",
    "    cost = -1/m*(np.sum(label*np.log(AL)))\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZ = np.array([[1,2,3],[4,5,6],[7,8,9]])\\nA = activation_function(Z, \"softmax\")\\nlabel = [[1,0,0],[0,1,0],[0,0,1]]\\ncost_function(A, label)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "label = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "cost_function(A, label)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(dA, A):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[A == 0] = 0\n",
    "    return dZ\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(A , label):\n",
    "    return A - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99764437  0.00235563  0.00235563]\n",
      " [ 0.04731416 -0.95268584  0.04731416]\n",
      " [ 0.95033021  0.95033021 -0.04966979]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "label = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "dA = softmax_grad(A, label)\n",
    "print(dA)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(dZ, A_prev, W ):\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m*(np.dot(dZ, A_prev.T))\n",
    "    db = 1/m*np.sum(dZ)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(no_of_layers, nL, X, Y, learning_rate, EPOCHS):\n",
    "\n",
    "    param= {}\n",
    "    results={\"A0\":X}\n",
    "    \n",
    "    Z = {}\n",
    "    #initializing parameters:\n",
    "    for l in range(1, no_of_layers):\n",
    "        param[\"W\" + str(l)], param[\"b\" + str(l)] = initialization(nL[l],nL[l-1])   \n",
    "        W = param[\"W\" + str(l)]\n",
    "        x = W.shape\n",
    "        print(\"W\"+str(l)+\" = \"+ str(x))\n",
    "\n",
    "    for epochs in range(EPOCHS):\n",
    "        for l in range(1, no_of_layers-1):\n",
    "            \n",
    "            #initialization of parameters \n",
    "            W = param[\"W\" + str(l)]\n",
    "            b = param[\"b\" + str(l)]\n",
    "\n",
    "            #forward propogation \n",
    "            Z[\"Z\"+ str(l)] = forward_prop(results[\"A\" + str(l-1)], W, b)\n",
    "            results[\"A\"+ str(l)] = activation_function( Z[\"Z\"+str(l)], activation=\"relu\")\n",
    "            ##################################\n",
    "            z = Z[\"Z\"+ str(l)] \n",
    "            As = results[\"A\"+ str(l)]\n",
    "            y = z.shape\n",
    "            a_shape = As.shape\n",
    "            print(\"Z\"+str(l)+\" = \"+ str(y))\n",
    "            print(\"A\"+str(l)+\" = \"+ str(a_shape))\n",
    "            ################################\n",
    "        Z[\"Z\" + str(no_of_layers-1)] = forward_prop(results[\"A\" + str(no_of_layers-2 )], param[\"W\"+str(no_of_layers-1)], param[\"b\"+str(no_of_layers-1)])\n",
    "        results[\"A\"+ str(no_of_layers-1)] = activation_function( Z[\"Z\"+ str(no_of_layers-1)], activation=\"sigmoid\")\n",
    "            ##############################\n",
    "        z = Z[\"Z\"+ str(no_of_layers-1)] \n",
    "        As = results[\"A\"+ str(no_of_layers-1)]\n",
    "        y = z.shape\n",
    "        a_shape = As.shape\n",
    "        print(\"Z\"+str(no_of_layers-1)+\" = \"+ str(y))\n",
    "        print(\"A\"+str(no_of_layers-1)+\" = \"+ str(a_shape))\n",
    "            #################################\n",
    "        #finding cost:\n",
    "\n",
    "        cost = cost_function(results[\"A\"+ str(no_of_layers-1)], Y)\n",
    "\n",
    "        print(cost)\n",
    "        #backward prop :\n",
    "        grads = {}\n",
    "\n",
    "        AL = results[\"A\"+ str(no_of_layers-1)]\n",
    "        print(AL.shape)\n",
    "        grads[\"dA\"+str(no_of_layers-1)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        #dZ = grads[\"dA\"+str(no_of_layers-1)]*sigmoid_grad(AL)\n",
    "        dZ = softmax_grad(AL , Y)\n",
    "        \n",
    "        ####################\n",
    "        print(\"dZ\"+str(no_of_layers-1)+\" = \"+ str(dZ.shape))\n",
    "        ###################\n",
    "        for l in range(no_of_layers-1,0,-1):\n",
    "            grads[\"dA\"+str(l-1)],  grads[\"dW\"+str(l)], grads[\"db\"+str(l)] = backward_prop(dZ, results[\"A\"+str(l-1)], param[\"W\"+str(l)] )\n",
    "            dZ = relu_grad(grads[\"dA\"+str(l-1)],results[\"A\"+str(l-1)])\n",
    "            #####################\n",
    "            shape1 = grads[\"dA\"+str(l-1)].shape\n",
    "            print(\"dA\"+str(l-1)+\" = \"+ str(shape1))\n",
    "            shape2 = grads[\"dW\"+str(l)].shape\n",
    "            print(\"dW\"+str(l)+\" = \"+ str(shape2))\n",
    "            shape3 = grads[\"db\"+str(l)].shape\n",
    "            print(\"db\"+str(l)+\" = \"+ str(shape3))\n",
    "            \n",
    "        # updating the parameters:\n",
    "\n",
    "        for l in range(1,no_of_layers):\n",
    "            param[\"W\"+str(l)] = param[\"W\"+str(l)] - learning_rate*grads[\"dW\"+str(l)]\n",
    "            param[\"b\"+str(l)] = param[\"b\"+str(l)] - learning_rate*grads[\"db\"+str(l)] \n",
    "            \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tf_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
