{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev)+b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(next_layer_dims, prev_layer_dims):\n",
    "    W  = np.random.randn(next_layer_dims, prev_layer_dims)*0.01\n",
    "    b = np.zeros([next_layer_dims,1])\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z, activation):\n",
    "    #if activation == 'sigmoid':\n",
    "      #  A = 1/(1+np.exp(-Z))\n",
    "\n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(Z, 0)\n",
    "        \n",
    "    if activation == 'softmax':\n",
    "        A = np.exp(Z)\n",
    "        sum = A.sum(axis = 0, keepdims=True)\n",
    "        A = A/sum\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00235563 0.00235563 0.00235563]\n",
      " [0.04731416 0.04731416 0.04731416]\n",
      " [0.95033021 0.95033021 0.95033021]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "print(A)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL, label):\n",
    "\n",
    "    m = AL.shape[1]\n",
    "   # cost = -1/m*(np.sum(label*np.log(AL)+(1-label)*np.log(1-AL)))\n",
    "    cost = -1/m*(np.sum(label*np.log(AL)))\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZ = np.array([[1,2,3],[4,5,6],[7,8,9]])\\nA = activation_function(Z, \"softmax\")\\nlabel = [[1,0,0],[0,1,0],[0,0,1]]\\ncost_function(A, label)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "label = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "cost_function(A, label)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(dA, A):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[A == 0] = 0\n",
    "    return dZ\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(A , label):\n",
    "    return A - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nZ = np.array([[1,2,3],[4,5,6],[7,8,9]])\\nA = activation_function(Z, \"softmax\")\\nlabel = [[1,0,0],[0,1,0],[0,0,1]]\\ndA = softmax_grad(A, label)\\nprint(dA)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Z = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "A = activation_function(Z, \"softmax\")\n",
    "label = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "dA = softmax_grad(A, label)\n",
    "print(dA)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(dZ, A_prev, W ):\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m*(np.dot(dZ, A_prev.T))\n",
    "    db = 1/m*np.sum(dZ)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(no_of_layers, nL, X, Y, learning_rate, EPOCHS):\n",
    "\n",
    "    param= {}\n",
    "    results={\"A0\":X}\n",
    "    \n",
    "    Z = {}\n",
    "    #initializing parameters:\n",
    "    for l in range(1, no_of_layers):\n",
    "        param[\"W\" + str(l)], param[\"b\" + str(l)] = initialization(nL[l],nL[l-1])   \n",
    "        W = param[\"W\" + str(l)]\n",
    "        x = W.shape\n",
    "        print(\"W\"+str(l)+\" = \"+ str(x))\n",
    "\n",
    "    for epochs in range(EPOCHS):\n",
    "        for l in range(1, no_of_layers-1):\n",
    "            \n",
    "            #initialization of parameters \n",
    "            W = param[\"W\" + str(l)]\n",
    "            b = param[\"b\" + str(l)]\n",
    "\n",
    "            #forward propogation \n",
    "            Z[\"Z\"+ str(l)] = forward_prop(results[\"A\" + str(l-1)], W, b)\n",
    "            results[\"A\"+ str(l)] = activation_function( Z[\"Z\"+str(l)], activation=\"relu\")\n",
    "            ##################################\n",
    "            '''\n",
    "            z = Z[\"Z\"+ str(l)] \n",
    "            As = results[\"A\"+ str(l)]\n",
    "            y = z.shape\n",
    "            a_shape = As.shape\n",
    "            print(\"Z\"+str(l)+\" = \"+ str(y))\n",
    "            print(\"A\"+str(l)+\" = \"+ str(a_shape))\n",
    "            '''\n",
    "                \n",
    "\n",
    "            ################################\n",
    "        Z[\"Z\" + str(no_of_layers-1)] = forward_prop(results[\"A\" + str(no_of_layers-2 )], param[\"W\"+str(no_of_layers-1)], param[\"b\"+str(no_of_layers-1)])\n",
    "        results[\"A\"+ str(no_of_layers-1)] = activation_function( Z[\"Z\"+ str(no_of_layers-1)], activation=\"softmax\")\n",
    "            ##############################\n",
    "        '''\n",
    "        z = Z[\"Z\"+ str(no_of_layers-1)] \n",
    "        As = results[\"A\"+ str(no_of_layers-1)]\n",
    "        y = z.shape\n",
    "        a_shape = As.shape\n",
    "        print(\"Z\"+str(no_of_layers-1)+\" = \"+ str(y))\n",
    "        print(\"A\"+str(no_of_layers-1)+\" = \"+ str(a_shape))\n",
    "        '''\n",
    "\n",
    "        \n",
    "\n",
    "            #################################\n",
    "        #finding cost:\n",
    "\n",
    "        cost = cost_function(results[\"A\"+ str(no_of_layers-1)], Y)\n",
    "\n",
    "        if epochs%500 == 0:\n",
    "            print(cost)\n",
    "      \n",
    "        #backward prop :\n",
    "        grads = {}\n",
    "\n",
    "        AL = results[\"A\"+ str(no_of_layers-1)]\n",
    "        #print(AL.shape)\n",
    "        grads[\"dA\"+str(no_of_layers-1)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        #dZ = grads[\"dA\"+str(no_of_layers-1)]*sigmoid_grad(AL)\n",
    "        dZ = softmax_grad(AL , Y)\n",
    "        \n",
    "        ####################\n",
    "        \n",
    "        #print(\"dZ\"+str(no_of_layers-1)+\" = \"+ str(dZ.shape))\n",
    "        \n",
    "        \n",
    "        ###################\n",
    "        for l in range(no_of_layers-1,0,-1):\n",
    "            grads[\"dA\"+str(l-1)],  grads[\"dW\"+str(l)], grads[\"db\"+str(l)] = backward_prop(dZ, results[\"A\"+str(l-1)], param[\"W\"+str(l)] )\n",
    "            dZ = relu_grad(grads[\"dA\"+str(l-1)],results[\"A\"+str(l-1)])\n",
    "            #####################\n",
    "            '''\n",
    "            shape1 = grads[\"dA\"+str(l-1)].shape\n",
    "            print(\"dA\"+str(l-1)+\" = \"+ str(shape1))\n",
    "            shape2 = grads[\"dW\"+str(l)].shape\n",
    "            print(\"dW\"+str(l)+\" = \"+ str(shape2))\n",
    "            shape3 = grads[\"db\"+str(l)].shape\n",
    "            print(\"db\"+str(l)+\" = \"+ str(shape3))\n",
    "            \n",
    "            '''\n",
    "           \n",
    "        # updating the parameters:\n",
    "\n",
    "        for l in range(1,no_of_layers):\n",
    "            param[\"W\"+str(l)] = param[\"W\"+str(l)] - learning_rate*grads[\"dW\"+str(l)]\n",
    "            param[\"b\"+str(l)] = param[\"b\"+str(l)] - learning_rate*grads[\"db\"+str(l)] \n",
    "            \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tf_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1080)\n",
      "1080\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "X_train = X_train_flatten/255.\n",
    "X_test = X_test_flatten/255.\n",
    "Y_train = np.zeros([5,Y_train_orig.shape[1]])\n",
    "print(Y_train.shape)\n",
    "print(Y_train.shape[1])\n",
    "for y in range(Y_train.shape[1]):\n",
    "    Y_train[Y_train_orig - 1,y] =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nL = [X_train.shape[0], 16, 32, 16, 5]\n",
    "params = training_model(5, nL, X_train, Y_train, learning_rate=0.1, EPOCHS=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(param,no_of_layers, nL, X, Y):\n",
    "    results={\"A0\":X}\n",
    "    \n",
    "    Z = {}\n",
    "    for l in range(1, no_of_layers-1):\n",
    "            \n",
    "            #initialization of parameters \n",
    "            W = param[\"W\" + str(l)]\n",
    "            b = param[\"b\" + str(l)]\n",
    "\n",
    "            #forward propogation \n",
    "            Z[\"Z\"+ str(l)] = forward_prop(results[\"A\" + str(l-1)], W, b)\n",
    "            results[\"A\"+ str(l)] = activation_function( Z[\"Z\"+str(l)], activation=\"relu\")\n",
    "            ##################################\n",
    "            z = Z[\"Z\"+ str(l)] \n",
    "            As = results[\"A\"+ str(l)]\n",
    "            y = z.shape\n",
    "            a_shape = As.shape\n",
    "            print(\"Z\"+str(l)+\" = \"+ str(y))\n",
    "            print(\"A\"+str(l)+\" = \"+ str(a_shape))\n",
    "            ################################\n",
    "    Z[\"Z\" + str(no_of_layers-1)] = forward_prop(results[\"A\" + str(no_of_layers-2 )], param[\"W\"+str(no_of_layers-1)], param[\"b\"+str(no_of_layers-1)])\n",
    "    results[\"A\"+ str(no_of_layers-1)] = activation_function( Z[\"Z\"+ str(no_of_layers-1)], activation=\"softmax\")\n",
    "            ##############################\n",
    "    z = Z[\"Z\"+ str(no_of_layers-1)] \n",
    "    As = results[\"A\"+ str(no_of_layers-1)]\n",
    "    y = z.shape\n",
    "    a_shape = As.shape\n",
    "    print(\"Z\"+str(no_of_layers-1)+\" = \"+ str(y))\n",
    "    print(\"A\"+str(no_of_layers-1)+\" = \"+ str(a_shape))\n",
    "            #################################\n",
    "        #finding cost:\n",
    "\n",
    "    cost = cost_function(results[\"A\"+ str(no_of_layers-1)], Y)\n",
    "\n",
    "    print(cost)\n",
    "    \n",
    "    # converting the results to one hot key\n",
    "    \n",
    "    AL = results[\"A\"+ str(no_of_layers-1)]\n",
    "    c = np.where(AL==Y)\n",
    "    correct_count = len(c)\n",
    "    print(correct_count)\n",
    "    accuracy = correct_count/Y.shape[1]\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 = (16, 1080)\n",
      "A1 = (16, 1080)\n",
      "Z2 = (32, 1080)\n",
      "A2 = (32, 1080)\n",
      "Z3 = (16, 1080)\n",
      "A3 = (16, 1080)\n",
      "Z4 = (5, 1080)\n",
      "A4 = (5, 1080)\n",
      "20.117973905949246\n",
      "2\n",
      "0.001851851851851852\n"
     ]
    }
   ],
   "source": [
    "predict(params,5, nL, X_train, Y_train_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "AL = np.array([[2,3],[5,6],[8,9]])\n",
    "\n",
    "A = np.zeros(AL.shape)\n",
    "t = np.argmax(AL, axis=0)\n",
    "index = 0 \n",
    "for i in t:\n",
    "    A[i, index] = 1 \n",
    "    index+=1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
